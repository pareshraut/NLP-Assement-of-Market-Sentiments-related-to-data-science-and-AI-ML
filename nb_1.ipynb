{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Class Final Project Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 16.4 s, total: 29.3 s\n",
      "Wall time: 5min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(199838, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_news_final_project = pd.read_parquet('https://storage.googleapis.com/msca-bdp-data-open/news_final_project/news_final_project.parquet', engine='pyarrow')\n",
    "df_news_final_project.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPUs: 8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_processors = multiprocessing.cpu_count()\n",
    "print(f'Available CPUs: {num_processors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 7 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(nb_workers=num_processors-1, use_memory_fs=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://auckland.scoop.co.nz/2020/01/aut-boosts-ai-expertise-with-new-ailab/</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>en</td>\n",
       "      <td>auckland.scoop.co.nz » AUT boosts AI expertise with new AiLab</td>\n",
       "      <td>\\n\\nauckland.scoop.co.nz » AUT boosts AI expertise with new AiLab\\nScoop Search\\nContact\\nNewsagent\\nLogin\\n \\n \\nSupercity\\nBusiness\\nEducation\\nEntertainment\\nHealth\\nPolice\\nPolitics\\n\\n\\n \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://en.people.cn/n3/2021/0318/c90000-9830122.html</td>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>en</td>\n",
       "      <td>Artificial intelligence improves parking efficiency in Chinese cities - People's Daily Online</td>\n",
       "      <td>\\n\\nArtificial intelligence improves parking efficiency in Chinese cities - People's Daily Online\\n\\nHome\\nChina Politics\\nForeign Affairs\\nOpinions\\nVideo: We Are China\\nBusiness\\nMilitary\\nWorld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://galusaustralis.com/2020/02/486473/legaltech-artificial-intelligence-market-2019-technology-advancement-and-future-scope-casetext-inc-catalyst-repository-systems-ebrevia/</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>en</td>\n",
       "      <td>LegalTech Artificial Intelligence Market 2019 Technology Advancement and Future Scope – Casetext Inc., Catalyst Repository Systems, eBREVIA – Galus Australis</td>\n",
       "      <td>LegalTech Artificial Intelligence Market 2019 Technology Advancement and Future Scope – Casetext Inc., Catalyst Repository Systems, eBREVIA – Galus Australis          \\n\\nGalus Australis\\n\\nBusine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://newsparliament.com/2020/02/27/children-with-autism-saw-their-learning-and-social-skills-boosted-after-playing-with-this-ai-robot/</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>en</td>\n",
       "      <td>Children With Autism Saw Their Learning and Social Skills Boosted After Playing With This AI Robot – News Parliament</td>\n",
       "      <td>\\nChildren With Autism Saw Their Learning and Social Skills Boosted After Playing With This AI Robot – News Parliament\\n \\n\\nSkip to content\\n\\t\\t\\tThursday, February 27, 2020\\t\\t\\n\\nLatest:\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://spaceref.com/astronomy/observation-simulation-and-ai-join-forces-to-reveal-a-clear-universe.html</td>\n",
       "      <td>2021-07-05</td>\n",
       "      <td>en</td>\n",
       "      <td>Observation, Simulation, And AI Join Forces To Reveal A Clear Universe - SpaceRef</td>\n",
       "      <td>\\n\\nObservation, Simulation, And AI Join Forces To Reveal A Clear Universe - SpaceRef\\n\\n \\nHome |\\nNASA Watch\\nSpaceRef Business\\nAstrobiology Web\\nAdvertising\\nAdd an Event\\nSign up for our Dail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199833</th>\n",
       "      <td>https://www.yourcentralvalley.com/news/tech-news/un-urges-moratorium-on-use-of-ai-that-imperils-human-rights/</td>\n",
       "      <td>2021-09-15</td>\n",
       "      <td>en</td>\n",
       "      <td>UN urges moratorium on use of AI that imperils human rights | YourCentralValley.com KSEE24 | CBS47</td>\n",
       "      <td>\\n\\n \\nUN urges moratorium on use of AI that imperils human rights | YourCentralValley.com KSEE24 | CBS47\\n\\nSkip to content\\n\\n\\nYourCentralValley.com\\nFresno, CA\\n\\n70°\\nSponsored By\\n\\n  \\nTogg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199834</th>\n",
       "      <td>https://www.zdnet.com/article/csiro-using-artificial-intelligence-to-map-1-7m-australian-grain-paddocks/</td>\n",
       "      <td>2020-08-07</td>\n",
       "      <td>en</td>\n",
       "      <td>CSIRO using artificial intelligence to map 1.7m Australian grain paddocks | ZDNet</td>\n",
       "      <td>\\n\\nCSIRO using artificial intelligence to map 1.7m Australian grain paddocks | ZDNet\\n\\n   \\n\\n                            Edition: \\n\\n            Asia\\n        \\n            Australia\\n        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199835</th>\n",
       "      <td>https://yemenwatch.com/escaping-the-singularity-why-artificial-intelligence-will-not-save-the-planet-18/</td>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>en</td>\n",
       "      <td>Escaping the Singularity: Why Artificial Intelligence Will Not Save the Planet | Yemen Watch</td>\n",
       "      <td>\\n\\nEscaping the Singularity: Why Artificial Intelligence Will Not Save the Planet | Yemen Watch\\nHome\\nNews\\nBusiness\\nMagazine\\nSports\\nCulture\\nPolitics\\n \\nSign in\\nWelcome!Log into your acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199836</th>\n",
       "      <td>https://yourstory.com/2022/12/google-ai-model-indian-languages-sundar-pichai-ashwini-vaishnaw</td>\n",
       "      <td>2022-12-19</td>\n",
       "      <td>en</td>\n",
       "      <td>Google building AI model to support over 100 Indian languages: Sundar Pichai</td>\n",
       "      <td>Google building AI model to support over 100 Indian languages: Sundar PichaiMenuGo to HomeStories &amp; MediaBrands of New IndiaSMB StoryYS GulfHerStorySocialStoryEnterprise StoryThe Decrypting StoryT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199837</th>\n",
       "      <td>https://yourstory.com/mystory/artificial-intelligence-machine-learning-customer-experience</td>\n",
       "      <td>2020-07-25</td>\n",
       "      <td>en</td>\n",
       "      <td>Artificial Intelligence + Machine Learning = Great Customer experience</td>\n",
       "      <td>Artificial Intelligence + Machine Learning = Great Customer experienceLOGINYourStoryEducationHerStorySocialStorySMBStoryYourStoryTVMoreCompaniesAdvertise With UsMakers-IndiaAutoStoryMyStoryWeekend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199838 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                     url  \\\n",
       "0                                                                                                            http://auckland.scoop.co.nz/2020/01/aut-boosts-ai-expertise-with-new-ailab/   \n",
       "1                                                                                                                                   http://en.people.cn/n3/2021/0318/c90000-9830122.html   \n",
       "2       http://galusaustralis.com/2020/02/486473/legaltech-artificial-intelligence-market-2019-technology-advancement-and-future-scope-casetext-inc-catalyst-repository-systems-ebrevia/   \n",
       "3                                               http://newsparliament.com/2020/02/27/children-with-autism-saw-their-learning-and-social-skills-boosted-after-playing-with-this-ai-robot/   \n",
       "4                                                                                http://spaceref.com/astronomy/observation-simulation-and-ai-join-forces-to-reveal-a-clear-universe.html   \n",
       "...                                                                                                                                                                                  ...   \n",
       "199833                                                                     https://www.yourcentralvalley.com/news/tech-news/un-urges-moratorium-on-use-of-ai-that-imperils-human-rights/   \n",
       "199834                                                                          https://www.zdnet.com/article/csiro-using-artificial-intelligence-to-map-1-7m-australian-grain-paddocks/   \n",
       "199835                                                                          https://yemenwatch.com/escaping-the-singularity-why-artificial-intelligence-will-not-save-the-planet-18/   \n",
       "199836                                                                                     https://yourstory.com/2022/12/google-ai-model-indian-languages-sundar-pichai-ashwini-vaishnaw   \n",
       "199837                                                                                        https://yourstory.com/mystory/artificial-intelligence-machine-learning-customer-experience   \n",
       "\n",
       "              date language  \\\n",
       "0       2020-01-28       en   \n",
       "1       2021-03-18       en   \n",
       "2       2020-02-26       en   \n",
       "3       2020-02-27       en   \n",
       "4       2021-07-05       en   \n",
       "...            ...      ...   \n",
       "199833  2021-09-15       en   \n",
       "199834  2020-08-07       en   \n",
       "199835  2021-01-15       en   \n",
       "199836  2022-12-19       en   \n",
       "199837  2020-07-25       en   \n",
       "\n",
       "                                                                                                                                                                title  \\\n",
       "0                                                                                                       auckland.scoop.co.nz » AUT boosts AI expertise with new AiLab   \n",
       "1                                                                       Artificial intelligence improves parking efficiency in Chinese cities - People's Daily Online   \n",
       "2       LegalTech Artificial Intelligence Market 2019 Technology Advancement and Future Scope – Casetext Inc., Catalyst Repository Systems, eBREVIA – Galus Australis   \n",
       "3                                                Children With Autism Saw Their Learning and Social Skills Boosted After Playing With This AI Robot – News Parliament   \n",
       "4                                                                                   Observation, Simulation, And AI Join Forces To Reveal A Clear Universe - SpaceRef   \n",
       "...                                                                                                                                                               ...   \n",
       "199833                                                             UN urges moratorium on use of AI that imperils human rights | YourCentralValley.com KSEE24 | CBS47   \n",
       "199834                                                                              CSIRO using artificial intelligence to map 1.7m Australian grain paddocks | ZDNet   \n",
       "199835                                                                   Escaping the Singularity: Why Artificial Intelligence Will Not Save the Planet | Yemen Watch   \n",
       "199836                                                                                   Google building AI model to support over 100 Indian languages: Sundar Pichai   \n",
       "199837                                                                                         Artificial Intelligence + Machine Learning = Great Customer experience   \n",
       "\n",
       "                                                                                                                                                                                                           text  \n",
       "0       \\n\\nauckland.scoop.co.nz » AUT boosts AI expertise with new AiLab\\nScoop Search\\nContact\\nNewsagent\\nLogin\\n \\n \\nSupercity\\nBusiness\\nEducation\\nEntertainment\\nHealth\\nPolice\\nPolitics\\n\\n\\n \\n\\n...  \n",
       "1       \\n\\nArtificial intelligence improves parking efficiency in Chinese cities - People's Daily Online\\n\\nHome\\nChina Politics\\nForeign Affairs\\nOpinions\\nVideo: We Are China\\nBusiness\\nMilitary\\nWorld...  \n",
       "2       LegalTech Artificial Intelligence Market 2019 Technology Advancement and Future Scope – Casetext Inc., Catalyst Repository Systems, eBREVIA – Galus Australis          \\n\\nGalus Australis\\n\\nBusine...  \n",
       "3       \\nChildren With Autism Saw Their Learning and Social Skills Boosted After Playing With This AI Robot – News Parliament\\n \\n\\nSkip to content\\n\\t\\t\\tThursday, February 27, 2020\\t\\t\\n\\nLatest:\\n\\n\\n...  \n",
       "4       \\n\\nObservation, Simulation, And AI Join Forces To Reveal A Clear Universe - SpaceRef\\n\\n \\nHome |\\nNASA Watch\\nSpaceRef Business\\nAstrobiology Web\\nAdvertising\\nAdd an Event\\nSign up for our Dail...  \n",
       "...                                                                                                                                                                                                         ...  \n",
       "199833  \\n\\n \\nUN urges moratorium on use of AI that imperils human rights | YourCentralValley.com KSEE24 | CBS47\\n\\nSkip to content\\n\\n\\nYourCentralValley.com\\nFresno, CA\\n\\n70°\\nSponsored By\\n\\n  \\nTogg...  \n",
       "199834  \\n\\nCSIRO using artificial intelligence to map 1.7m Australian grain paddocks | ZDNet\\n\\n   \\n\\n                            Edition: \\n\\n            Asia\\n        \\n            Australia\\n        ...  \n",
       "199835   \\n\\nEscaping the Singularity: Why Artificial Intelligence Will Not Save the Planet | Yemen Watch\\nHome\\nNews\\nBusiness\\nMagazine\\nSports\\nCulture\\nPolitics\\n \\nSign in\\nWelcome!Log into your acco...  \n",
       "199836  Google building AI model to support over 100 Indian languages: Sundar PichaiMenuGo to HomeStories & MediaBrands of New IndiaSMB StoryYS GulfHerStorySocialStoryEnterprise StoryThe Decrypting StoryT...  \n",
       "199837  Artificial Intelligence + Machine Learning = Great Customer experienceLOGINYourStoryEducationHerStorySocialStorySMBStoryYourStoryTVMoreCompaniesAdvertise With UsMakers-IndiaAutoStoryMyStoryWeekend...  \n",
       "\n",
       "[199838 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news_final_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = df_news_final_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 56.7 s, total: 1min 8s\n",
      "Wall time: 8min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news_df[\"clean_text\"] = news_df[\"text\"].parallel_apply(lambda x: re.sub(r'\\n', '',x)) # remove new line\n",
    "news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(r'https://.*', '',x)) # remove url\n",
    "news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(r'@\\w*', '',x)) # remove mention\n",
    "news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(r'#\\w*', '',x)) # remove tag\n",
    "# news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(r'[^a-zA-Z.\\s]', '',x)) # keep only letters, periods, and white space\n",
    "news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(' +', ' ',x)) # change consecutive white space into 1 whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 60 s, total: 1min 13s\n",
      "Wall time: 7min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(r'\\xa0SAP', '',x))\n",
    "news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(r'\\xa0National', '',x))\n",
    "news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(r'\\xa0', '',x))\n",
    "news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(r'\\t', '',x))\n",
    "news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(r'\\r', '',x))\n",
    "news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(r'\\|', '',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nObservation, Simulation, And AI Join Forces To Reveal A Clear Universe - SpaceRef\\n\\n \\nHome |\\nNASA Watch\\nSpaceRef Business\\nAstrobiology Web\\nAdvertising\\nAdd an Event\\nSign up for our Daily Newsletter \\n\\n \\n\\n\\nInternational Space Station\\nNASA Hack Space\\nCalendar\\nMissions\\nSpace Weather \\n             \\xa0             \\n           \\nObservation, Simulation, And AI Join Forces To Reveal A Clear Universe\\n\\n\\n             Press Release - Source: NATIONAL INSTITUTES OF NATURAL SCIENCES\\n             \\n            \\nPosted July  4, 2021 10:00 PM\\n View Comments\\nUsing AI driven data analysis to peel back the noise and find the actual shape of the Universe. CREDIT The Institute of Statistical Mathematics\\nJapanese astronomers have developed a new artificial intelligence (AI) technique to remove noise in astronomical data due to random variations in galaxy shapes.\\nAfter extensive training and testing on large mock data created by supercomputer simulations, they then applied this new tool to actual data from Japan\\'s Subaru Telescope and found that the mass distribution derived from using this method is consistent with the currently accepted models of the Universe. This is a powerful new tool for analyzing big data from current and planned astronomy surveys.\\nWide area survey data can be used to study the large-scale structure of the Universe through measurements of gravitational lensing patterns. In gravitational lensing, the gravity of a foreground object, like a cluster of galaxies, can distort the image of a background object, such as a more distant galaxy. Some examples of gravitational lensing are obvious, such as the \"Eye of Horus\". The large-scale structure, consisting mostly of mysterious \"dark\" matter, can distort the shapes of distant galaxies as well, but the expected lensing effect is subtle. Averaging over many galaxies in an area is required to create a map of foreground dark matter distributions.\\nBut this technique of looking at many galaxy images runs into a problem; some galaxies are just innately a little funny looking. It is difficult to distinguish between a galaxy image distorted by gravitational lensing and a galaxy that is actually distorted. This is referred to as shape noise and is one of the limiting factors in research studying the large-scale structure of the Universe.\\nTo compensate for shape noise, a team of Japanese astronomers first used ATERUI II, the world\\'s most powerful supercomputer dedicated to astronomy, to generate 25,000 mock galaxy catalogs based on real data from the Subaru Telescope. They then added realist noise to these perfectly known artificial data sets, and trained an AI to statistically recover the lensing dark matter from the mock data.\\nAfter training, the AI was able to recover previously unobservable fine details, helping to improve our understanding of the cosmic dark matter. Then using this AI on real data covering 21 square degrees of the sky, the team found a distribution of foreground mass consistent with the standard cosmological model.\\n\"This research shows the benefits of combining different types of research: observations, simulations, and AI data analysis.\" comments Masato Shirasaki, the leader of the team, \"In this era of big data, we need to step across traditional boundaries between specialties and use all available tools to understand the data. If we can do this, it will open new fields in astronomy and other sciences.\"\\n###\\nThese results appeared as Shirasaki et al. \"Noise reduction for weak lensing mass mapping: an application of generative adversarial networks to Subaru Hyper Suprime-Cam first-year data\" in the June 2021 issue of Monthly Notices of the Royal Astronomical Society.\\nPlease follow SpaceRef on Twitter and Like us on Facebook.\\n\\nTAGS: AStronomy\\nFILED UNDER: Astronomy\\nSOURCE: NATIONAL INSTITUTES OF NATURAL SCIENCES\\n                     Press Release\\n                    \\n            \\n\\nTweet\\n \\nObservation, Simulation, And AI Join Forces To Reveal A Clear Universe\\nJapanese astronomers have developed a new artificial intelligence (AI) technique to remove noise in astronomical data due to random variations in galaxy shapes.\\n\\n\\nPlease enable JavaScript to view the comments powered by Disqus.\\n\\nFollow @SpaceRef\\n\\n\\xa0\\n\\n\\nCalendar\\n\\nEvents\\nLaunches\\nYour Event\\n14 Jul: The Challenges of Managing Small Space Flight Projects\\n15 Jul: NASA Aerospace Safety Advisory Panel Meeting\\n19 Jul: NfoLD/NExSS Standards of Evidence for Life Detection Community Workshop\\n * Submit Your Event\\xa0\\xa0|\\xa0\\xa0More Events *\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tNo events for the next 2 days.\\n\\t\\t\\t\\t\\t\\t\\t * Submit Your Event\\xa0\\xa0|\\xa0\\xa0More Launches *\\n\\n\\nAre you hosting an event? We accept all space related events in our calendar and all it takes is about 5 minutes for your to fill out the online event form. Let us help you get the word out about your event. Submit your event today.\\n\\xa0                \\n\\nRecent Articles\\n\\nObservation, Simulation, And AI Join Forces To Reveal A Clear Universe\\nNASA Weekly ISS Space to Ground Report for 2 July, 2021\\nNASA Space Station On-Orbit Status 1 July, 2021 - Russian Progress 78 Spacecraft Arrives\\nEarth from Space: North Frisian Islands\\nThe Red Sea As Viewed From Space\\nEarth\\'s Cryosphere Is Shrinking By 87,000 Square Kilometers Per Year\\nPlant Water Management Experiment On ISS\\nClosing The Gap On The Missing Lithium\\nNASA Space Station On-Orbit Status 30 June, 2021 - Physics and Biology Studies\\nCygnus Cargo Droid Departs The ISS\\nSubscribe\\n\\n\\n RSS\\n Twitter\\n Facebook\\n Google+\\n UStream\\n YouTube\\n Vimeo\\n Newsletter\\n \\n\\n\\nMasthead\\n\\nTip your editorstips@spaceref.com\\nSenior Editor & Chief Architect:Marc BoucherEmail | Twitter\\nEditor-in-Chief:Keith CowingEmail | Twitter \\n\\n \\n\\n\\nCompany Information\\n\\t\\tAbout SpaceRef\\nManagement\\nContact Information\\nAdvertising\\nSpaceRef RSS - XML News Feeds\\nCompany Press Releases\\nEmployment\\nCopyright Notice\\nPrivacy Policy\\nTerms of Use\\n\\n\\nSpaceRef Network\\n\\t\\t\\tSpaceRef\\nNASA Watch\\nSpaceRef Business\\nAstrobiology Web\\n\\n\\nArchives\\n\\t\\tNews Archives\\nPress Releases\\nStatus Reports\\nEurope\\nAsia\\n\\n\\nFeatured Topics\\n\\t\\tNASA Hack Space\\nHubble\\nKepler\\nJames Webb Telescope\\nLunar Reconnaissance Orbiter\\nNew Horizons\\n Copyright © 2021 SpaceRef Interactive Inc. All rights reserved.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.text[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Observation, Simulation, And AI Join Forces To Reveal A Clear Universe - SpaceRef Home NASA WatchSpaceRef BusinessAstrobiology WebAdvertisingAdd an EventSign up for our Daily Newsletter International Space StationNASA Hack SpaceCalendarMissionsSpace Weather  Observation, Simulation, And AI Join Forces To Reveal A Clear Universe Press Release - Source: NATIONAL INSTITUTES OF NATURAL SCIENCES Posted July 4, 2021 10:00 PM View CommentsUsing AI driven data analysis to peel back the noise and find the actual shape of the Universe. CREDIT The Institute of Statistical MathematicsJapanese astronomers have developed a new artificial intelligence (AI) technique to remove noise in astronomical data due to random variations in galaxy shapes.After extensive training and testing on large mock data created by supercomputer simulations, they then applied this new tool to actual data from Japan\\'s Subaru Telescope and found that the mass distribution derived from using this method is consistent with the currently accepted models of the Universe. This is a powerful new tool for analyzing big data from current and planned astronomy surveys.Wide area survey data can be used to study the large-scale structure of the Universe through measurements of gravitational lensing patterns. In gravitational lensing, the gravity of a foreground object, like a cluster of galaxies, can distort the image of a background object, such as a more distant galaxy. Some examples of gravitational lensing are obvious, such as the \"Eye of Horus\". The large-scale structure, consisting mostly of mysterious \"dark\" matter, can distort the shapes of distant galaxies as well, but the expected lensing effect is subtle. Averaging over many galaxies in an area is required to create a map of foreground dark matter distributions.But this technique of looking at many galaxy images runs into a problem; some galaxies are just innately a little funny looking. It is difficult to distinguish between a galaxy image distorted by gravitational lensing and a galaxy that is actually distorted. This is referred to as shape noise and is one of the limiting factors in research studying the large-scale structure of the Universe.To compensate for shape noise, a team of Japanese astronomers first used ATERUI II, the world\\'s most powerful supercomputer dedicated to astronomy, to generate 25,000 mock galaxy catalogs based on real data from the Subaru Telescope. They then added realist noise to these perfectly known artificial data sets, and trained an AI to statistically recover the lensing dark matter from the mock data.After training, the AI was able to recover previously unobservable fine details, helping to improve our understanding of the cosmic dark matter. Then using this AI on real data covering 21 square degrees of the sky, the team found a distribution of foreground mass consistent with the standard cosmological model.\"This research shows the benefits of combining different types of research: observations, simulations, and AI data analysis.\" comments Masato Shirasaki, the leader of the team, \"In this era of big data, we need to step across traditional boundaries between specialties and use all available tools to understand the data. If we can do this, it will open new fields in astronomy and other sciences.\" results appeared as Shirasaki et al. \"Noise reduction for weak lensing mass mapping: an application of generative adversarial networks to Subaru Hyper Suprime-Cam first-year data\" in the June 2021 issue of Monthly Notices of the Royal Astronomical Society.Please follow SpaceRef on Twitter and Like us on Facebook.TAGS: AStronomyFILED UNDER: AstronomySOURCE: NATIONAL INSTITUTES OF NATURAL SCIENCES Press Release Tweet Observation, Simulation, And AI Join Forces To Reveal A Clear UniverseJapanese astronomers have developed a new artificial intelligence (AI) technique to remove noise in astronomical data due to random variations in galaxy shapes.Please enable JavaScript to view the comments powered by Disqus.Follow CalendarEventsLaunchesYour Event14 Jul: The Challenges of Managing Small Space Flight Projects15 Jul: NASA Aerospace Safety Advisory Panel Meeting19 Jul: NfoLD/NExSS Standards of Evidence for Life Detection Community Workshop * Submit Your EventMore Events *No events for the next 2 days. * Submit Your EventMore Launches *Are you hosting an event? We accept all space related events in our calendar and all it takes is about 5 minutes for your to fill out the online event form. Let us help you get the word out about your event. Submit your event today. Recent ArticlesObservation, Simulation, And AI Join Forces To Reveal A Clear UniverseNASA Weekly ISS Space to Ground Report for 2 July, 2021NASA Space Station On-Orbit Status 1 July, 2021 - Russian Progress 78 Spacecraft ArrivesEarth from Space: North Frisian IslandsThe Red Sea As Viewed From SpaceEarth\\'s Cryosphere Is Shrinking By 87,000 Square Kilometers Per YearPlant Water Management Experiment On ISSClosing The Gap On The Missing LithiumNASA Space Station On-Orbit Status 30 June, 2021 - Physics and Biology StudiesCygnus Cargo Droid Departs The ISSSubscribe RSS Twitter Facebook Google+ UStream YouTube Vimeo Newsletter MastheadTip your editorstips.comSenior Editor & Chief Architect:Marc BoucherEmail  TwitterEditor-in-Chief:Keith CowingEmail  Twitter Company InformationAbout SpaceRefManagementContact InformationAdvertisingSpaceRef RSS - XML News FeedsCompany Press ReleasesEmploymentCopyright NoticePrivacy PolicyTerms of UseSpaceRef NetworkSpaceRefNASA WatchSpaceRef BusinessAstrobiology WebArchivesNews ArchivesPress ReleasesStatus ReportsEuropeAsiaFeatured TopicsNASA Hack SpaceHubbleKeplerJames Webb TelescopeLunar Reconnaissance OrbiterNew Horizons Copyright © 2021 SpaceRef Interactive Inc. All rights reserved.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.clean_text[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 377 ms, sys: 682 ms, total: 1.06 s\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news_df[\"clean_title\"] = news_df[\"title\"].parallel_apply(lambda x: re.sub(r'\\n', '',x)) # remove new line\n",
    "news_df[\"clean_title\"] = news_df[\"clean_title\"].parallel_apply(lambda x: re.sub(r'https://.*', '',x)) # remove url\n",
    "news_df[\"clean_title\"] = news_df[\"clean_title\"].parallel_apply(lambda x: re.sub(r'@\\w*', '',x)) # remove mention\n",
    "news_df[\"clean_title\"] = news_df[\"clean_title\"].parallel_apply(lambda x: re.sub(r'#\\w*', '',x)) # remove tag\n",
    "# news_df[\"clean_text\"] = news_df[\"clean_text\"].parallel_apply(lambda x: re.sub(r'[^a-zA-Z.\\s]', '',x)) # keep only letters, periods, and white space\n",
    "news_df[\"clean_title\"] = news_df[\"clean_title\"].parallel_apply(lambda x: re.sub(' +', ' ',x)) # change consecutive white space into 1 whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 442 ms, sys: 473 ms, total: 915 ms\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "news_df[\"clean_title\"] = news_df[\"clean_title\"].parallel_apply(lambda x: re.sub(r'\\xa0SAP', '',x))\n",
    "news_df[\"clean_title\"] = news_df[\"clean_title\"].parallel_apply(lambda x: re.sub(r'\\xa0National', '',x))\n",
    "news_df[\"clean_title\"] = news_df[\"clean_title\"].parallel_apply(lambda x: re.sub(r'\\xa0', '',x))\n",
    "news_df[\"clean_title\"] = news_df[\"clean_title\"].parallel_apply(lambda x: re.sub(r'\\t', '',x))\n",
    "news_df[\"clean_title\"] = news_df[\"clean_title\"].parallel_apply(lambda x: re.sub(r'\\r', '',x))\n",
    "news_df[\"clean_title\"] = news_df[\"clean_title\"].parallel_apply(lambda x: re.sub(r'\\|', '',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.35 ms, sys: 289 µs, total: 8.64 ms\n",
      "Wall time: 6.68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "top_N = 100\n",
    "\n",
    "clean_text = news_df['clean_text']\n",
    "\n",
    "clean_text = str(clean_text)\n",
    "\n",
    "\n",
    "# Use TweetTokenizer\n",
    "tweet_tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "words = tweet_tokenizer.tokenize(clean_text)\n",
    "\n",
    "#stopwords = stopwords.words('english')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "words = [word for word in words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "words = [word for word in words if not word.isnumeric()]\n",
    "\n",
    "# Remove punctuation\n",
    "# words = [word for word in words if word.isalpha()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "# words = [word.lower() for word in words]\n",
    "\n",
    "# Remove stopwords\n",
    "words = [word for word in words if word not in stopwords]\n",
    "\n",
    "bgs = nltk.bigrams(words)\n",
    "\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist_2 = nltk.FreqDist(bgs)\n",
    "\n",
    "fdist_2_df = pd.DataFrame(fdist_2.most_common(),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n",
    "# fdist_2_df.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Artificial, Intelligence)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(boosts, AI)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(AI, expertise)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(expertise, new)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(..., Artificial)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(auckland.scoop.co.nz, AUT)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(AUT, boosts)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(new, AiLabScoop)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(AiLabScoop, SearchContactNewsagentLogin)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(SearchContactNewsagentLogin, SupercityBusinessEducationEntertainmentHealthPolicePolitics)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(SupercityBusinessEducationEntertainmentHealthPolicePolitics, TweetAUT)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(TweetAUT, boosts)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(new, AiL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(AiL, ...)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(Artificial, intelligence)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(intelligence, improves)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(improves, parking)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(parking, efficiency)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(efficiency, Chinese)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(Chinese, cities)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(cities, People's)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                          Word  \\\n",
       "0                                                                   (Artificial, Intelligence)   \n",
       "1                                                                                 (boosts, AI)   \n",
       "2                                                                              (AI, expertise)   \n",
       "3                                                                             (expertise, new)   \n",
       "4                                                                            (..., Artificial)   \n",
       "5                                                                  (auckland.scoop.co.nz, AUT)   \n",
       "6                                                                                (AUT, boosts)   \n",
       "7                                                                            (new, AiLabScoop)   \n",
       "8                                                    (AiLabScoop, SearchContactNewsagentLogin)   \n",
       "9   (SearchContactNewsagentLogin, SupercityBusinessEducationEntertainmentHealthPolicePolitics)   \n",
       "10                     (SupercityBusinessEducationEntertainmentHealthPolicePolitics, TweetAUT)   \n",
       "11                                                                          (TweetAUT, boosts)   \n",
       "12                                                                                  (new, AiL)   \n",
       "13                                                                                  (AiL, ...)   \n",
       "14                                                                  (Artificial, intelligence)   \n",
       "15                                                                    (intelligence, improves)   \n",
       "16                                                                         (improves, parking)   \n",
       "17                                                                       (parking, efficiency)   \n",
       "18                                                                       (efficiency, Chinese)   \n",
       "19                                                                           (Chinese, cities)   \n",
       "20                                                                          (cities, People's)   \n",
       "\n",
       "    Frequency  \n",
       "0           3  \n",
       "1           2  \n",
       "2           2  \n",
       "3           2  \n",
       "4           2  \n",
       "5           1  \n",
       "6           1  \n",
       "7           1  \n",
       "8           1  \n",
       "9           1  \n",
       "10          1  \n",
       "11          1  \n",
       "12          1  \n",
       "13          1  \n",
       "14          1  \n",
       "15          1  \n",
       "16          1  \n",
       "17          1  \n",
       "18          1  \n",
       "19          1  \n",
       "20          1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_2_df.head(n=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.3 ms, sys: 6.07 ms, total: 7.37 ms\n",
      "Wall time: 5.88 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "top_N = 100\n",
    "\n",
    "clean_title = news_df['clean_title']\n",
    "\n",
    "clean_title = str(clean_title)\n",
    "\n",
    "\n",
    "# Use TweetTokenizer\n",
    "tweet_tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "words = tweet_tokenizer.tokenize(clean_title)\n",
    "\n",
    "#stopwords = stopwords.words('english')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Remove single-character tokens (mostly punctuation)\n",
    "words = [word for word in words if len(word) > 1]\n",
    "\n",
    "# Remove numbers\n",
    "words = [word for word in words if not word.isnumeric()]\n",
    "\n",
    "# Remove punctuation\n",
    "# words = [word for word in words if word.isalpha()]\n",
    "\n",
    "# Lowercase all words (default_stopwords are lowercase too)\n",
    "# words = [word.lower() for word in words]\n",
    "\n",
    "# Remove stopwords\n",
    "words = [word for word in words if word not in stopwords]\n",
    "\n",
    "bgs = nltk.bigrams(words)\n",
    "\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist_2 = nltk.FreqDist(bgs)\n",
    "\n",
    "fdist_2_df = pd.DataFrame(fdist_2.most_common(),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "\n",
    "# fdist_2_df.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Artificial, Intelligence)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(auckland.scoop.co.nz, AUT)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(AUT, boosts)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(boosts, AI)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(AI, expertise)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(expertise, new)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(new, AiLab)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(AiLab, Artificial)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Artificial, intelligence)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(intelligence, improves)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(improves, parking)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(parking, efficiency)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(efficiency, Chinese)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(Chinese, cities)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(cities, People's)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(People's, Daily)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(Daily, Online)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(Online, LegalTech)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(LegalTech, Artificial)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(Intelligence, Market)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Word  Frequency\n",
       "0    (Artificial, Intelligence)          3\n",
       "1   (auckland.scoop.co.nz, AUT)          1\n",
       "2                 (AUT, boosts)          1\n",
       "3                  (boosts, AI)          1\n",
       "4               (AI, expertise)          1\n",
       "5              (expertise, new)          1\n",
       "6                  (new, AiLab)          1\n",
       "7           (AiLab, Artificial)          1\n",
       "8    (Artificial, intelligence)          1\n",
       "9      (intelligence, improves)          1\n",
       "10          (improves, parking)          1\n",
       "11        (parking, efficiency)          1\n",
       "12        (efficiency, Chinese)          1\n",
       "13            (Chinese, cities)          1\n",
       "14           (cities, People's)          1\n",
       "15            (People's, Daily)          1\n",
       "16              (Daily, Online)          1\n",
       "17          (Online, LegalTech)          1\n",
       "18      (LegalTech, Artificial)          1\n",
       "19       (Intelligence, Market)          1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_2_df.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AI', 5),\n",
       " ('Artificial', 4),\n",
       " ('Intelligence', 3),\n",
       " ('intelligence', 2),\n",
       " ('With', 2),\n",
       " ('Learning', 2),\n",
       " ('auckland.scoop.co.nz', 1),\n",
       " ('AUT', 1),\n",
       " ('boosts', 1),\n",
       " ('expertise', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(words)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('...', 9),\n",
       " ('AI', 6),\n",
       " ('Artificial', 4),\n",
       " ('ZDNet', 4),\n",
       " ('Intelligence', 3),\n",
       " ('With', 3),\n",
       " ('boosts', 2),\n",
       " ('expertise', 2),\n",
       " ('new', 2),\n",
       " ('intelligence', 2),\n",
       " ('Daily', 2),\n",
       " ('Galus', 2),\n",
       " ('Learning', 2),\n",
       " ('United', 2),\n",
       " ('auckland.scoop.co.nz', 1),\n",
       " ('AUT', 1),\n",
       " ('AiLabScoop', 1),\n",
       " ('SearchContactNewsagentLogin', 1),\n",
       " ('SupercityBusinessEducationEntertainmentHealthPolicePolitics', 1),\n",
       " ('TweetAUT', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(words)\n",
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ranges from 2020-01-01 to 2023-02-07\n"
     ]
    }
   ],
   "source": [
    "min_date = news_df['date'].min()\n",
    "max_date = news_df['date'].max()\n",
    "\n",
    "print('Data ranges from {} to {}'.format(min_date, max_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186913, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = ['Data Science', 'data science', 'DATA SCIENCE','AI', 'ai', 'artificial intelligence','Artificial Intelligence','ARTIFICIAL INTELLIGENCE', 'ML', 'NLP', \n",
    "            'Artificial General Intelligence','Chatbots', 'AI Marketplaces', 'Intelligent Applications', \n",
    "           'Augmented Intelligence', 'Decision Intelligence', 'AI Cloud Services', 'cloud services', 'GPU Accelerators', 'Computer Vision','Deep Neural Network',\n",
    "           'Deep Learning','Cognitive Computing','Autonomous Vehicles','Knowledge Graphs', 'Responsible AI', 'Machine Customers', 'Decision Intelligence', 'Autonomous Vehicles',\n",
    "           'Human-Centered AI', 'AI Governance', 'Natural Language Processing', 'Machine Learning', 'Smart Robots', 'Operational AI Systems' , 'Data-Centric AI', 'AI TRiSM', 'Generative AI',\n",
    "           'Responsible AI']\n",
    "query = '|'.join(keywords)\n",
    "news_df = news_df[news_df['clean_text'].str.contains(query)]\n",
    "news_df = news_df[news_df['clean_title'].str.contains(query)]\n",
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df[[\"date\",\"clean_title\",\"clean_text\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.to_parquet(\"nlp_clean_text.parquet\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
